{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8aa72b-1afa-4da8-b4f6-f34b99806212",
   "metadata": {},
   "source": [
    "# NLP Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504cd65-d7c9-4a90-a76c-92db83007e57",
   "metadata": {},
   "source": [
    "## Домашнее задание номер 2\n",
    "\n",
    "На последнем семинаре мы проанализировали несколько различных морфологических теггеров. Как же узнать, какой использовать? Давайте сравним их качество!\n",
    "\n",
    "В этой домашке вам будет нужно найти тексты на русском языке (размер корпуса не менее 200 слов), \n",
    "в которых  будут какие-то трудные или неоднозначные для POS теггинга моменты и разметить их вручную \n",
    "– с помощью этих текстов мы будем оценивать качество работы наших теггеров. В текстах размечаем только части речи, ничего больше!\n",
    "1. (1 балл) Создание, разметка корпуса и объяснение того, почему этот текст подходит для оценки (какие моменты вы тут считаете трудными для автоматического посттеггинга и почему, в этом вам может помочь второй ридинг). Не забывайте, что разные теггеры могут использовать разные тегсеты: напишите комментарий о том, какой тегсет вы берёте для разметки и почему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ec3c9b-5e8b-46c1-95e3-ed11fee11615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# garden path sentences gathered from a google search\n",
    "with open('corpus.txt', 'r') as f:\n",
    "    corp = f.read()\n",
    "    \n",
    "len(corp.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f109388-c1d9-4dd3-b77b-9b410eca202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# i will then manually attach POS tags to every token\n",
    "corp_underscored = ' '.join([token+'_' for token in word_tokenize(corp)])\n",
    "\n",
    "with open('corpus_annotated.txt', 'w') as f:\n",
    "    f.write(corp_underscored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f8a039-103c-4bf7-8d2c-42749cefebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus_annotated.txt', 'r') as f:\n",
    "    corp_annotated = f.read()\n",
    "\n",
    "tag_true = [token.split('_')[1] for token in corp_annotated.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5318e56-3876-4332-9024-11f1a284761d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADJ',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'AUX',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'PUNCT',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'PUNCT',\n",
       " 'PROPN',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'PUNCT',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'PRON',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'ADJ',\n",
       " 'PUNCT',\n",
       " 'PROPN',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'AUX',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'PUNCT',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'PART',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'PRON',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'ADV',\n",
       " 'ADP',\n",
       " 'VERB',\n",
       " 'PRON',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'DET',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'ADV',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'PROPN',\n",
       " 'AUX',\n",
       " 'ADV',\n",
       " 'ADV',\n",
       " 'VERB',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADV',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'PROPN',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'PRON',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'PRON',\n",
       " 'ADV',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'PRON',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'ADV',\n",
       " 'ADJ',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'VERB',\n",
       " 'ADJ',\n",
       " 'CCONJ',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'CCONJ',\n",
       " 'PRON',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d8927-f9ac-4c61-8d60-608f225328fa",
   "metadata": {},
   "source": [
    "2. (3 балла) Потом вам будет нужно взять три  POS теггера для русского языка (udpipe, stanza, natasha, pymorphy, mystem, spacy, deeppavlov) и «прогнать» текст через каждый из них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97d663c-9263-4ce6-8c0b-1823f8d111d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages')\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548af605-66ec-4d3a-b9e6-5f55965aa158",
   "metadata": {},
   "source": [
    "3. (2 балла) Затем оценим accuracy для каждого теггера. Заметьте, что в разных системах имена тегов и части речи  могут отличаться, – вам надо будет свести это всё к единому стандарту с помощью какой-то функции-конвертера и сравнить с вашим размеченным руками эталоном - тоже с помощью какого-то кода или функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6405bc52-e87d-4d62-b85a-f2ec78894f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk2stanza = {'$': 'SYM',\n",
    " '#': 'SYM',\n",
    " \"''\": 'PUNCT',\n",
    " '(': 'PUNCT',\n",
    " ')': 'PUNCT',\n",
    " ',': 'PUNCT',\n",
    " '--': 'PUNCT',\n",
    " '.': 'PUNCT',\n",
    " ':': 'PUNCT',\n",
    " 'CC': 'CONJ',\n",
    " 'CD': 'NUM',\n",
    " 'DT': 'DET',\n",
    " 'EX': 'PRON',\n",
    " 'FW': 'X',\n",
    " 'IN': 'ADP',\n",
    " 'JJ': 'ADJ',\n",
    " 'JJR': 'ADJ',\n",
    " 'JJS': 'ADJ',\n",
    " 'LS': 'SYM',\n",
    " 'MD': 'AUX',\n",
    " 'NN': 'NOUN',\n",
    " 'NNP': 'PROPN',\n",
    " 'NNPS': 'PROPN',\n",
    " 'NNS': 'NOUN',\n",
    " 'PDT': 'DET',\n",
    " 'POS': 'X',\n",
    " 'PRP': 'PRON',\n",
    " 'PRP$': 'PRON',\n",
    " 'RB': 'ADV',\n",
    " 'RBR': 'ADV',\n",
    " 'RBS': 'ADV',\n",
    " 'RP': 'ADV',\n",
    " 'SYM': 'SYM',\n",
    " 'TO': 'ADP',\n",
    " 'UH': 'INTJ',\n",
    " 'VB': 'VERB',\n",
    " 'VBD': 'VERB',\n",
    " 'VBG': 'VERB',\n",
    " 'VBN': 'VERB',\n",
    " 'VBP': 'VERB',\n",
    " 'VBZ': 'VERB',\n",
    " 'WDT': 'PRON',\n",
    " 'WP': 'PRON',\n",
    " 'WP$': 'PRON',\n",
    " 'WRB': 'PRON',\n",
    " '``': 'PUNCT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01137c96-195e-444e-bfaf-ac5acfc53dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_seq(seq, model_type, model=None):\n",
    "    \n",
    "    if model_type == 'spacy':\n",
    "        if not model:\n",
    "            model = spacy.load(\"en_core_web_sm\")\n",
    "        return [(token.text, token.pos_) for token in model(seq)]\n",
    "    elif model_type == 'nltk':\n",
    "        text = word_tokenize(seq)\n",
    "        nltk_tagged_seq = nltk.pos_tag(text)\n",
    "        return [(token[0], nltk2stanza[token[1]]) for token in nltk_tagged_seq]\n",
    "    elif model_type == 'stanza':\n",
    "        if not model:\n",
    "            model = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos')\n",
    "        doc = model(seq)\n",
    "        return [(token.text, token.upos) for sent in doc.sentences for token in sent.words]\n",
    "\n",
    "class MorphTagger():\n",
    "    \n",
    "    \"\"\"\n",
    "    a class for morhological tagging of English text\n",
    "    args:\n",
    "        model -- callable which tags a sequence (spacy/nltk/stanza)\n",
    "        model_type -- string in ['spacy', 'nltk', 'stanza']\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_type,\n",
    "                 model=None):\n",
    "        \n",
    "        self.model = model\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def load_model(self):\n",
    "        \n",
    "        if not self.model:\n",
    "            if self.model_type == 'spacy':\n",
    "                self.model = spacy.load(\"en_core_web_sm\")\n",
    "            elif self.model_type == 'stanza':\n",
    "                self.model = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos')\n",
    "        \n",
    "    def tag_seq(self, seq):\n",
    "        \n",
    "        return tag_seq(seq, self.model_type, self.model)\n",
    "    \n",
    "    def predict(self, seq):\n",
    "        \n",
    "        return [token[1] for token in tag_seq(seq, self.model_type, self.model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b80c43-f041-4527-bd59-40c332d3ca8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON', 'VERB', 'NOUN', 'ADV', 'ADV', 'ADV', 'PRON', 'AUX', 'PART']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = MorphTagger('spacy')\n",
    "tagger.load_model()\n",
    "tagger.predict('i love trains so much literally i can\\'t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a820da-7636-48bd-a558-4bb6b425040b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 21:47:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6d09f5e6fb4e3185fe24ece7ba0492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 21:47:13 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2022-10-11 21:47:13 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2022-10-11 21:47:13 INFO: Use device: cpu\n",
      "2022-10-11 21:47:13 INFO: Loading: tokenize\n",
      "2022-10-11 21:47:13 INFO: Loading: pos\n",
      "2022-10-11 21:47:13 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "spacy_tagger = MorphTagger('spacy')\n",
    "spacy_tagger.load_model()\n",
    "\n",
    "nltk_tagger = MorphTagger('nltk')\n",
    "\n",
    "stanza_tagger = MorphTagger('stanza')\n",
    "stanza_tagger.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c5a8b28-467f-413f-8aea-090472f87f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus_annotated.txt', 'r') as f:\n",
    "    corpus = f.read()\n",
    "    corpus_list = [token.split('_')[0] for token in corp_annotated.split(' ')]\n",
    "    corpus_clean = ' '.join(corpus_list)\n",
    "\n",
    "# spacy and stanza do their own tokenization, which is not to our benefit\n",
    "tag_pred_spacy = [spacy_tagger.predict(token)[0] for token in corpus_list]\n",
    "tag_pred_nltk = nltk_tagger.predict(corpus_clean)\n",
    "tag_pred_stanza = [stanza_tagger.predict(token)[0] for token in corpus_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "946cacef-b8f4-466d-9de9-6c6e67b6c80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247 247 247\n"
     ]
    }
   ],
   "source": [
    "# убедимся что везде одинаково по длине\n",
    "print(len(tag_pred_spacy), len(tag_pred_nltk), len(tag_pred_stanza))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8137d8ba-a53b-4d56-9745-ad0fa2bfcf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>true</th>\n",
       "      <th>spacy</th>\n",
       "      <th>nltk</th>\n",
       "      <th>stanza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>man</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cotton</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>grows</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>drink</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Have</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>supplementary</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>out</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>sank</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>bit</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token  true spacy  nltk stanza\n",
       "10            man  VERB  NOUN  NOUN   NOUN\n",
       "23         cotton   ADJ  NOUN  NOUN   NOUN\n",
       "25             is   AUX   AUX  VERB   VERB\n",
       "28          grows  VERB  VERB  NOUN   VERB\n",
       "34          drink  VERB  VERB  NOUN   VERB\n",
       "39           Have   AUX  VERB  VERB   VERB\n",
       "48  supplementary  NOUN   ADJ   ADJ    ADJ\n",
       "62            out   ADP   ADP   ADV    ADV\n",
       "72           sank  VERB  VERB  NOUN   VERB\n",
       "87            bit  VERB  NOUN  NOUN   NOUN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preds = pd.DataFrame({\n",
    "    'token': corpus_list,\n",
    "    'true': tag_true,\n",
    "    'spacy': tag_pred_spacy,\n",
    "    'nltk': tag_pred_nltk,\n",
    "    'stanza': tag_pred_stanza\n",
    "})\n",
    "\n",
    "preds.loc[preds['true'] != preds['nltk']].head(10)\n",
    "# интересно посмотреть, где они ошибаются. зачастую недопонимают garden path предложения так, как и задумано\n",
    "# (например, man -- NOUN в предложении the old man the boat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db1fc219-72fc-4f19-a862-97fc4f0a98d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy accuracy:  0.631578947368421\n",
      "nltk accuracy:  0.8582995951417004\n",
      "stanza accuracy:  0.7854251012145749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('spacy accuracy: ', accuracy_score(tag_true, tag_pred_spacy))\n",
    "print('nltk accuracy: ', accuracy_score(tag_true, tag_pred_nltk))\n",
    "print('stanza accuracy: ', accuracy_score(tag_true, tag_pred_stanza))\n",
    "\n",
    "# победил nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e584e-be0a-4db1-8c80-04b690000e51",
   "metadata": {},
   "source": [
    "4. (2 балла) Дальше вам нужно взять лучший теггер для русского языка и с его помощью написать функцию (chunker), которая выделяет из размеченного текста 3 типа n-грамм, соответствующих какому-то шаблону (к примеру не + какая-то часть речи или NP или сущ.+ наречие и тд) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aad0cc10-1b18-4c77-8a1f-d38a0c032160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(seq, pattern):\n",
    "    \n",
    "    \"\"\"\n",
    "    chunker function\n",
    "    args:\n",
    "        seq -- sequence to search for chunks\n",
    "        pattern -- list of POS tags to find\n",
    "    \"\"\"\n",
    "    \n",
    "    text = word_tokenize(seq)\n",
    "    nltk_tagged_seq = nltk.pos_tag(text)\n",
    "    tagged_seq = [(token[0], nltk2stanza[token[1]]) for token in nltk_tagged_seq]\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(len(tagged_seq)-len(pattern)+1):\n",
    "        n_gram = tagged_seq[i:i+len(pattern)]\n",
    "        if all([n_gram[i][1] == pattern[i] for i in range(len(pattern))]):\n",
    "            chunks.append(n_gram)\n",
    "            \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bc8cd67-6fee-4c93-a656-a8d5a8624224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('i', 'NOUN'), ('love', 'VERB')], [('i', 'NOUN'), ('am', 'VERB')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_chunks('i love trains, i love trains so much i am going to drive a train one day', ['NOUN', 'VERB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60230f7-eaae-4a9e-bb3f-6890d515badf",
   "metadata": {},
   "source": [
    "5. (2-3 балла) В предыдущем дз многие из вас справедливо заметили, что если бы мы могли класть в словарь не только отдельные слова, но и словосочетания, то программа работала бы лучше. Предложите 3 шаблона (слово + POS-тег / POS-тег + POS-тег) запись которых в словарь, по вашему мнению, улучшила бы качество работы программы из предыдущей домашки. Балл за объяснение того, почему именно эти группы вы взяли, балл за встраивание функции в программу из предыдущей домашки, балл за сравнение качества предсказания тональности с улучшением и без (это бонусный одиннадцатый балл)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cb636ea-bc7f-4f46-a424-f893b3db2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_json('weezer_reviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37dd33bd-0e00-4d2c-8079-fc999335c540",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>grade</th>\n",
       "      <th>album</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Weezer has had so many duds over the last 20 years, even their good entries (The White Album, Everything Will be Alright in the End) end with middle of the pack ratings. Well, what if Weezer made a great album, again? Would anyone admit it? This is Weezer's first great album in 20+ years but I'm betting reviewers will play it safe and this will settle into the \"pretty good\" zone. But makeWeezer has had so many duds over the last 20 years, even their good entries (The White Album, Everything Will be Alright in the End) end with middle of the pack ratings. Well, what if Weezer made a great album, again? Would anyone admit it? This is Weezer's first great album in 20+ years but I'm betting reviewers will play it safe and this will settle into the \"pretty good\" zone. But make no mistake, it's reputation will grow. Substantively, it's a love letter to late Beatles music, something that sounds somewhat fresh again after rock has wandered in a different (mostly awful) direction over the last decade. Rock music hasn't used strings so effectively since peak Arcade Fire. Anyway, really welcome.… Expand</td>\n",
       "      <td>9.0</td>\n",
       "      <td>ok-human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>One of the Best Albums of the 90's and one of the most influential records of this generation...So many bands today have drawn from this epic album.  Its so raw and emotional yet enjoyable to listen to.  It truly tugs at the emotions of our inner selves.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>pinkerton-deluxe-edition</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>Always been underrated,weezer's second lp is their best and make me cry thinking of their last three to five records.Pinkerton,it's a classic,make me think of the time when it was released for the first time,sweet sweet memories!</td>\n",
       "      <td>8.0</td>\n",
       "      <td>pinkerton-deluxe-edition</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>most critics don't give the album more than a couple listens, and this record needs quite a few before its beauty is revealed. give it a fe more spins and you'll notice that \"This is Such a Pity\", \"The Damage In Your Heart\", \"The Other Way\", \"Freak Me Out\", and \"Haunt You Everyday\" are great additions to the Cuomo songbook.</td>\n",
       "      <td>7.0</td>\n",
       "      <td>make-believe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>By far their best effort since the 1990's. it's so great to have Weezer back in full form! Congratulations Rivers and the rest of the band on such an amazing album</td>\n",
       "      <td>9.0</td>\n",
       "      <td>everything-will-be-alright-in-the-end</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "68   Weezer has had so many duds over the last 20 years, even their good entries (The White Album, Everything Will be Alright in the End) end with middle of the pack ratings. Well, what if Weezer made a great album, again? Would anyone admit it? This is Weezer's first great album in 20+ years but I'm betting reviewers will play it safe and this will settle into the \"pretty good\" zone. But makeWeezer has had so many duds over the last 20 years, even their good entries (The White Album, Everything Will be Alright in the End) end with middle of the pack ratings. Well, what if Weezer made a great album, again? Would anyone admit it? This is Weezer's first great album in 20+ years but I'm betting reviewers will play it safe and this will settle into the \"pretty good\" zone. But make no mistake, it's reputation will grow. Substantively, it's a love letter to late Beatles music, something that sounds somewhat fresh again after rock has wandered in a different (mostly awful) direction over the last decade. Rock music hasn't used strings so effectively since peak Arcade Fire. Anyway, really welcome.… Expand   \n",
       "428                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         One of the Best Albums of the 90's and one of the most influential records of this generation...So many bands today have drawn from this epic album.  Its so raw and emotional yet enjoyable to listen to.  It truly tugs at the emotions of our inner selves.   \n",
       "437                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Always been underrated,weezer's second lp is their best and make me cry thinking of their last three to five records.Pinkerton,it's a classic,make me think of the time when it was released for the first time,sweet sweet memories!   \n",
       "675                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  most critics don't give the album more than a couple listens, and this record needs quite a few before its beauty is revealed. give it a fe more spins and you'll notice that \"This is Such a Pity\", \"The Damage In Your Heart\", \"The Other Way\", \"Freak Me Out\", and \"Haunt You Everyday\" are great additions to the Cuomo songbook.   \n",
       "347                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    By far their best effort since the 1990's. it's so great to have Weezer back in full form! Congratulations Rivers and the rest of the band on such an amazing album   \n",
       "\n",
       "     grade                                  album  target  \n",
       "68     9.0                               ok-human       1  \n",
       "428   10.0               pinkerton-deluxe-edition       1  \n",
       "437    8.0               pinkerton-deluxe-edition       1  \n",
       "675    7.0                           make-believe       1  \n",
       "347    9.0  everything-will-be-alright-in-the-end       1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "reviews.loc[reviews['target'] == 1].sample(5)\n",
    "\n",
    "# можно поисследовать отзывы, запуская эту ячейку"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72158466-474e-4087-bfe4-0a587e89ba8b",
   "metadata": {},
   "source": [
    "Сейчас найдем паттерны, посчитаем их частотность в каждом отзыве и будем добавлять к выходу функции predict() с весом, который подберем потом (потому что кто его знает, что окажется важнее -- слова в словаре или параметры)\n",
    "\n",
    "Искать будем следующее:\n",
    "\n",
    "- конструкции типа it's ADJ/DET NOUN -- могут встречаться в особенно эмоциональных отзывах\n",
    "- сочетание ADV+ADJ -- опять же, сигнализирует о наличии в отзыве оценочных слов, может помочь классификатору\n",
    "- сочетание DET+NOUN+VERB -- хотелось бы искать this/the album is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eed69d87-8f33-40fe-8437-1d637af80da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 851/851 [00:03<00:00, 224.95it/s]\n",
      "100%|██████████| 851/851 [00:03<00:00, 232.94it/s]\n",
      "100%|██████████| 851/851 [00:04<00:00, 180.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "reviews['adv+adj'] = reviews['text'].progress_apply(lambda x: len(get_chunks(x, ['ADV', 'ADJ'])))\n",
    "reviews['pron+verb'] = reviews['text'].progress_apply(lambda x: len(get_chunks(x, ['PRON', 'VERB'])))\n",
    "reviews['det+noun+verb'] = reviews['text'].progress_apply(lambda x: len(get_chunks(x, ['DET', 'NOUN', 'VERB'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f4fcf16-bb52-4764-bb80-40c974deef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# taken from HW1; optimized good/bad words dicts\n",
    "with open('dicts.json', 'r') as f:\n",
    "    dicts = json.load(f)\n",
    "    \n",
    "words_good_only = dicts['good']\n",
    "words_bad_only = dicts['bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee1d9e3-e6a7-4fd4-8178-1367499053dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(s, return_list=True):\n",
    "    # lower and tokenize\n",
    "    tokens = wordpunct_tokenize(s.lower())\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        # skip numbers and punctuation\n",
    "        if not (all([char in string.punctuation+'!….' for char in token]) or all([char.isdigit() for char in token])):\n",
    "            lemmas.append(lemmatizer.lemmatize(token))\n",
    "        \n",
    "    if return_list:   \n",
    "        return lemmas\n",
    "    else:\n",
    "        return ' '.join(lemmas)\n",
    "\n",
    "def get_sums(text, preprocess=True):\n",
    "    \n",
    "    if preprocess:\n",
    "        prep_text = preprocess_text(text)\n",
    "        \n",
    "    good_sum = sum([word in words_good_only for word in prep_text])\n",
    "    bad_sum = sum([word in words_bad_only for word in prep_text])\n",
    "    sum_dic = {1: good_sum, 0: bad_sum}\n",
    "    \n",
    "    return sum_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bf06855-b978-4d9b-9026-a996d45915a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 851/851 [00:03<00:00, 266.98it/s]\n",
      "100%|██████████| 851/851 [00:01<00:00, 647.25it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews['good_sum'] = reviews['text'].progress_apply(lambda x: get_sums(x)[1])\n",
    "reviews['bad_sum'] = reviews['text'].progress_apply(lambda x: get_sums(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759fe63-c222-4017-9a44-61a2fa2b9065",
   "metadata": {},
   "source": [
    "### попробуем сначала без паттернов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcc9c639-b398-42c2-9a46-b164e215a92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680 training reviews\n",
      "171 testing reviews\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% train, 10% validation, 10% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reviews[['good_sum', 'bad_sum']], \n",
    "    reviews[['target']], test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(X_train), 'training reviews')\n",
    "print(len(X_test), 'testing reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4eca4fb-68e1-4d0f-aa8c-5bce91bc9c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train f1: 0.9413793103448277\n",
      "test f1: 0.9072847682119205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shikunova/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "print('train f1:', f1_score(y_train, y_train_pred))\n",
    "print('test f1:', f1_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3d23c-a26d-4282-817b-17fdf078b9c2",
   "metadata": {},
   "source": [
    "### теперь с паттернами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1e99893-7ce3-4eba-8164-b907a0c3856d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train f1: 0.9420916162489196\n",
      "test f1: 0.9133333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shikunova/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% train, 10% validation, 10% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reviews[['good_sum', 'bad_sum', 'adv+adj', 'pron+verb', 'det+noun+verb']], \n",
    "    reviews[['target']], test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "print('train f1:', f1_score(y_train, y_train_pred))\n",
    "print('test f1:', f1_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bccbae-434f-4175-92a6-9cc307f112fb",
   "metadata": {},
   "source": [
    "Итого получается только самую малось получше. Еще можем посмотреть на веса, которые линейная модель присваивает нашим паттернам -- чем выше по модулю вес, тем важнее признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25030867-68f0-4255-a110-e26d6e97fc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good_sum': 0.48854698157778187,\n",
       " 'bad_sum': -1.3436067360744364,\n",
       " 'adv+adj': 0.06368291569361036,\n",
       " 'pron+verb': -0.14386032249299202,\n",
       " 'det+noun+verb': -0.16250793819456286}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(X_train.columns, list(logreg.coef_[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bde6377-ee54-45fa-94a2-c7ab2bebb35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'logreg weights'}>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('logreg weights')\n",
    "sns.barplot(x=X_train.columns, y=logreg.coef_[0])\n",
    "\n",
    "# паттерны для модели не очень важны, но видимо pron+verb и det+noun+verb встречаются скорее в негативных отзывах"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
